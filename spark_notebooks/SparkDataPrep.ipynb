{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to preparing data in PySpark\n",
    "\n",
    "My tutorial of preparing data in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rich/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a schema\n",
    "\n",
    "Creating a defined schema helps with data quality and import performance. Create a simple schema to read in the following columns:\n",
    "\n",
    "    Name\n",
    "    Age\n",
    "    City\n",
    "\n",
    "The Name and City columns are StringType() and the Age column is an IntegerType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new schema using the StructType method and a StructField for each field\n",
    "\n",
    "people_schema = StructType([\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city',StringType(),False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lazy processing\n",
    "\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. **This is due to Spark not performing any transformations until an action is requested.**\n",
    "\n",
    "Here define Data Frame (aa_dfw_df) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('dataprep').getOrCreate()\n",
    "\n",
    "import timeit\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('./data/AA_DFW_2017_Departures_Short.csv.gz')\n",
    "\n",
    "start_time1 = timeit.default_timer()\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower((aa_dfw_df['Destination Airport'])))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "elapsed1 = timeit.default_timer() - start_time1\n",
    "\n",
    "start_time2 = timeit.default_timer()\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()\n",
    "\n",
    "elapsed2 = timeit.default_timer() - start_time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007525733002694324"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09953547900659032"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a DataFrame in Parquet format\n",
    "\n",
    "The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "Creat a new Parquet file and then process some data from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'data/AA_DFW_2016_Departures_Short.csv'\n",
    "df1 = spark.read.csv(file_path1,sep=',',header=False,inferSchema=True,nullValue='NA')\n",
    "\n",
    "file_path2 = 'data/AA_DFW_2017_Departures_Short.csv'\n",
    "df2 = spark.read.csv(file_path2, sep=',',header=False,inferSchema=True,nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+--------------------+\n",
      "|              _c0|          _c1|                _c2|                 _c3|\n",
      "+-----------------+-------------+-------------------+--------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed ti...|\n",
      "|       01/01/2016|         0005|                HNL|                 529|\n",
      "|       01/01/2016|         0007|                OGG|                 512|\n",
      "|       01/01/2016|         0025|                PHL|                 161|\n",
      "|       01/01/2016|         0037|                SFO|                 259|\n",
      "+-----------------+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+--------------------+\n",
      "|              _c0|          _c1|                _c2|                 _c3|\n",
      "+-----------------+-------------+-------------------+--------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed ti...|\n",
      "|       01/01/2017|         0005|                HNL|                 537|\n",
      "|       01/01/2017|         0007|                OGG|                 498|\n",
      "|       01/01/2017|         0037|                SFO|                 241|\n",
      "|       01/01/2017|         0043|                DTW|                 134|\n",
      "+-----------------+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 140605\n",
      "df2 Count: 139359\n"
     ]
    }
   ],
   "source": [
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Parquet\n",
    "\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "Read in the Parquet file and register it as a SQL table. Run a quick query against the table (aka, the Parquet file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "#rename some columns for clarity\n",
    "flights_df = flights_df.selectExpr(\"_c0 as _c0\",\"_c1 as _c1\",\"_c2 as _c2\",\"_c3 as flight_duration\")\n",
    "#flights_df.show()\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering column content with Python\n",
    "\n",
    "The DataFrame voter_df contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Clean this data and remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'data/DallasCouncilVoters.csv'\n",
    "voter_df = spark.read.csv(file_path1,sep=',',header=True,inferSchema=True,nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(5, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.where('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "#voter_df.show()\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns\n",
    "\n",
    "Create two new columns - first_name and last_name. Split the VOTER_NAME column into words on any space character. Treat the last word as the last_name, and all other words as the first_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when() example\n",
    "\n",
    "The when() clause lets you conditionally modify a Data Frame based on its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When / Otherwise\n",
    "\n",
    "Add multiple values based on the voter's position. Modify your voter_df DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2) \n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show(5)\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show(5)\n",
    "\n",
    "#this aint working correctly look at again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using user defined functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "\n",
    "voter_df = voter_df.drop('first_name')\n",
    "voter_df = voter_df.drop('splits')\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an ID Field\n",
    "\n",
    "Find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "voter_df = voter_df.drop('random_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(Header=True).load('./data/DallasCouncilVoters.csv.gz')\n",
    "\n",
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER_NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
