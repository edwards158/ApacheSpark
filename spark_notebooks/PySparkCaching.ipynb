{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rich/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('caching').getOrCreate()\n",
    "\n",
    "import timeit\n",
    "\n",
    "# Load the CSV file\n",
    "departures_df = spark.read.format('csv').options(Header=True).load('./data/AA_DFW_2017_Departures_Short.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "|       01/01/2017|         0043|                DTW|                          134|\n",
      "|       01/01/2017|         0051|                STL|                           88|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching a DataFrame\n",
    "\n",
    "Assume caching can improve performance when reusing DataFrames and would like to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 8.546151 seconds\n",
      "Counting 139358 rows again took 1.414361 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have applied the caching transformation, it doesn't take effect until an action is run. The action instantiates the caching after the distinct() function completes. The second time, there is no need to recalculate anything so it returns almost immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache\n",
    "\n",
    "Remove the DataFrame from the cache to prevent any excess memory usage on your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import performance\n",
    "\n",
    "Take copy of same file and rename.  using * should improve import speed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t157199\n",
      "Time to run: 0.636141\n",
      "Total rows in split DataFrame:\t157199\n",
      "Time to run: 0.435705\n"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('./data/AA_DFW_2014_Departures_Short.csv.gz')\n",
    "split_df = spark.read.csv('./data/AA_DFW_2014_Departures_Short_0*.csv.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using split files runs more quickly than using one large file for import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: caching\n",
      "Driver TCP port: 37219\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark configurations\n",
    "\n",
    "Modify some of the settings to tune Spark to your needs. Import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 200\n",
      "Partition count after change: 500\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('./data/AA_DFW_2017_Departures_Short.csv.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+---+\n",
      "|       _c0| _c1|_c2|_c3|\n",
      "+----------+----+---+---+\n",
      "|01/01/2017|2332|ORD|119|\n",
      "|01/01/2017|2583|DTW|143|\n",
      "|01/02/2017|1010|STL|109|\n",
      "|01/02/2017|2333|ORF|182|\n",
      "|01/04/2017|1459|MCI| 87|\n",
      "|01/04/2017|2589|AUS| 73|\n",
      "|01/07/2017|1209|SMF|248|\n",
      "|01/08/2017|2655|LBB| 56|\n",
      "|01/09/2017|1414|AUS| 54|\n",
      "|01/09/2017|2555|MCO|152|\n",
      "|01/12/2017|0320|SAT| 65|\n",
      "|01/13/2017|2302|MSY| 75|\n",
      "|01/17/2017|0542|PHX|158|\n",
      "|01/17/2017|2195|JAC|159|\n",
      "|01/19/2017|2322|PBI|152|\n",
      "|01/22/2017|1568|MCI| 85|\n",
      "|01/22/2017|1606|RDU|158|\n",
      "|01/22/2017|2361|BWI|182|\n",
      "|01/22/2017|2396|OMA|120|\n",
      "|01/25/2017|0702|PHL|163|\n",
      "+----------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
