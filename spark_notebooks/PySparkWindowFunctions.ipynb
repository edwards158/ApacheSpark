{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rich/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL table from a dataframe\n",
    "\n",
    "A dataframe can be used to create a temporary table. A temporary table is one that will not exist after the session ends. Spark documentation also refers to this type of table as a SQL temporary view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"./data/trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.createOrReplaceTempView('table1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[train_id: string, station: string, time: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from table1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|train_id| station| time|\n",
      "+--------+--------+-----+\n",
      "|     324|San Jose|9:05a|\n",
      "|     217|San Jose|6:59a|\n",
      "+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from table1 where station = 'San Jose'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running sums using window function SQL\n",
    "\n",
    "A window function is like an aggregate function, except that it gives an output for every row in the dataset instead of a single row per group.\n",
    "\n",
    "You can do aggregation along with window functions. A running sum using a window function is simpler than what is required using joins. The query duration can also be much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/sched1.txt\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('schedule')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here query that adds an additional column to the records in this dataset called running_total. The column running_total SUM()s the difference between station time given by the diff_min column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+-------------+\n",
      "|train_id|      station| time|diff_min|running_total|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "|     217|       Gilroy|6:06a|     9.0|          9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|         15.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|         30.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|         36.0|\n",
      "|     217|      Capitol|6:42a|     8.0|         44.0|\n",
      "|     217|       Tamien|6:50a|     9.0|         53.0|\n",
      "|     217|     San Jose|6:59a|    null|         53.0|\n",
      "|     324|San Francisco|7:59a|     4.0|          4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|         17.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|         25.0|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it takes 53 minutes to go from Gilroy to San Jose by train 217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add time to next train column using window function and add row column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------+-----+---------+\n",
      "|row|train_id|     station| time|time_next|\n",
      "+---+--------+------------+-----+---------+\n",
      "|  1|     217|      Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|  San Martin|6:15a|    6:21a|\n",
      "|  3|     217| Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217|Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|     Capitol|6:42a|    6:50a|\n",
      "+---+--------+------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing dot notation with SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|    6:06a|    6:59a|\n",
      "|     324|    7:59a|    9:05a|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|    6:59a|\n",
      "|     324|    9:05a|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "# these two commands give the same result\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think aggregation is simpler to use in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate first and last times for each train line\n",
    "\n",
    "from pyspark.sql.functions import min, max, col\n",
    "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+---------+\n",
      "|train_id|      station| time|diff_min|time_next|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "|     217|       Gilroy|6:06a|     9.0|    6:15a|\n",
      "|     217|   San Martin|6:15a|     6.0|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|     6.0|    6:42a|\n",
      "|     217|      Capitol|6:42a|     8.0|    6:50a|\n",
      "|     217|       Tamien|6:50a|     9.0|    6:59a|\n",
      "|     217|     San Jose|6:59a|    null|     null|\n",
      "|     324|San Francisco|7:59a|     4.0|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    13.0|    8:16a|\n",
      "|     324|     Millbrae|8:16a|     8.0|    8:24a|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#window function sql\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT *, \n",
    "LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+---------+\n",
      "|train_id|      station| time|diff_min|time_next|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "|     217|       Gilroy|6:06a|     9.0|    6:15a|\n",
      "|     217|   San Martin|6:15a|     6.0|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|     6.0|    6:42a|\n",
      "|     217|      Capitol|6:42a|     8.0|    6:50a|\n",
      "|     217|       Tamien|6:50a|     9.0|    6:59a|\n",
      "|     217|     San Jose|6:59a|    null|     null|\n",
      "|     324|San Francisco|7:59a|     4.0|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    13.0|    8:16a|\n",
      "|     324|     Millbrae|8:16a|     8.0|    8:24a|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,lead,unix_timestamp\n",
    "\n",
    "#same as above window function using dot notation\n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time'))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column to a train schedule so that each row contains the number of minutes \n",
    "#for the train to reach its next stop. \n",
    "\n",
    "#using dot notation\n",
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dot_df = df.withColumn('diff_min', \n",
    "                    (unix_timestamp(lead('time', 1).over(window),'H:m') \n",
    "                     - unix_timestamp('time', 'H:m'))/60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL query to obtain an identical result to dot_df \n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has made two diff min columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
